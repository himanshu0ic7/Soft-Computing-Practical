{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> <b>LAB- 4</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-S_dL8Me5qp"
   },
   "source": [
    "## Aim: Implement logic gates AND, OR, NOR, NAND, XOR, and XNOR using perceptron\n",
    "## Theory:\n",
    "In this experiment, we use a perceptron model to implement basic logic gates, including AND, OR, NOR, NAND, XOR, and XNOR. The perceptron is a simple linear classifier used in machine learning, which calculates a weighted sum of its inputs and applies an activation function (in this case, the unit step function) to produce an output.\n",
    "\n",
    "The following components are involved:\n",
    "1. **Perceptron Model:**\n",
    "   - The perceptron model is implemented using a weighted sum and a bias term.\n",
    "   - The activation function used is a unit step function that returns 1 if the weighted sum is greater than or equal to zero, otherwise returns 0.\n",
    "\n",
    "2. **Logic Gates Implementation:**\n",
    "   - **AND Gate:** Uses weights `[1, 1]` and bias `-1.5`.\n",
    "   - **OR Gate:** Uses weights `[1, 1]` and bias `-0.5`.\n",
    "   - **NOT Gate:** Uses weight `-1` and bias `0.5`.\n",
    "   - **NOR Gate:** Combines the OR and NOT gates.\n",
    "   - **NAND Gate:** Combines the AND and NOT gates.\n",
    "   - **XOR Gate:** Combines the AND, OR, and NOT gates in a sequence to produce the exclusive OR output.\n",
    "   - **XNOR Gate:** Combines the OR, AND, and NOT gates in a different sequence to achieve the exclusive NOR output.\n",
    "\n",
    "3. **Testing the Perceptron Model:**\n",
    "   - Various input combinations are tested for each logic gate to validate the model.\n",
    "   - Outputs are generated for the logic gates using the perceptron, and results are displayed for all possible input pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14929,
     "status": "ok",
     "timestamp": 1727324134991,
     "user": {
      "displayName": "Punisher",
      "userId": "05926598485535198845"
     },
     "user_tz": -330
    },
    "id": "94VImEZDlNCK",
    "outputId": "212470d6-a62f-42eb-c0c9-b41905bcfb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter inputs (separate by spaces): 1 2 3 4 5\n",
      "inputs given to perceptron: [1. 2. 3. 4. 5.]\n",
      "Enter weights (separate by spaces): 0.1 0.1 0.1 0.1 0.1\n",
      "weights given to perceptron: [0.1 0.1 0.1 0.1 0.1]\n",
      "Enter the value of bias: 2\n",
      "the output using UnitStep function will be:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def unitStep(v):\n",
    "  if v >= 0:\n",
    "    return 1\n",
    "  else:\n",
    "      return 0\n",
    "\n",
    "def perceptronModel(x, w, b):\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unitStep(v)\n",
    "    return y\n",
    "\n",
    "def get_array(name):\n",
    "    elements = input(f\"Enter {name} (separate by spaces): \")\n",
    "    array = elements.split()\n",
    "    array=np.array(array,dtype=float)\n",
    "    return array\n",
    "\n",
    "array1 = get_array(\"inputs\")\n",
    "print(f\"inputs given to perceptron: {array1}\")\n",
    "array2 = get_array(\"weights\")\n",
    "print(f\"weights given to perceptron: {array2}\")\n",
    "b=int(input(\"Enter the value of bias: \"))\n",
    "a=perceptronModel(array1,array2,b)\n",
    "print(\"the output using UnitStep function will be: \",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1727324476546,
     "user": {
      "displayName": "Punisher",
      "userId": "05926598485535198845"
     },
     "user_tz": -330
    },
    "id": "4ult8bT-n5Zv",
    "outputId": "04bc751d-a453-4cc9-a610-26d19830acf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(0, 1) = 0\n",
      "AND(1, 1) = 1\n",
      "AND(0, 0) = 0\n",
      "AND(1, 0) = 0\n",
      "OR(0, 1) = 1\n",
      "OR(1, 1) = 1\n",
      "OR(0, 0) = 0\n",
      "OR(1, 0) = 1\n",
      "NOR(0, 1) = 0\n",
      "NOR(1, 1) = 0\n",
      "NOR(0, 0) = 1\n",
      "NOR(1, 0) = 0\n",
      "NAND(0, 1) = 1\n",
      "NAND(1, 1) = 0\n",
      "NAND(0, 0) = 1\n",
      "NAND(1, 0) = 1\n",
      "XOR(0, 1) = 1\n",
      "XOR(1, 1) = 0\n",
      "XOR(0, 0) = 0\n",
      "XOR(1, 0) = 1\n",
      "XNOR(0, 1) = 0\n",
      "XNOR(1, 1) = 1\n",
      "XNOR(0, 0) = 1\n",
      "XNOR(1, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "def AND_logicFunction(x):\n",
    "  w = np.array([1, 1])\n",
    "  b = -1.5\n",
    "  return perceptronModel(x, w, b)\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_logicFunction(test1)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_logicFunction(test2)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_logicFunction(test3)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_logicFunction(test4)))\n",
    "\n",
    "# OR Logic Function\n",
    "# w1 = 1, w2 = 1, b = -0.5\n",
    "def OR_logicFunction(x):\n",
    "  w = np.array([1, 1])\n",
    "  b = -0.5\n",
    "  return perceptronModel(x, w, b)\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_logicFunction(test1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_logicFunction(test2)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_logicFunction(test3)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_logicFunction(test4)))\n",
    "\n",
    "# NOT Logic Function\n",
    "# wNOT = -1, bNOT = 0.5\n",
    "def NOT_logicFunction(x):\n",
    "  wNOT = -1\n",
    "  bNOT = 0.5\n",
    "  return perceptronModel(x, wNOT, bNOT)\n",
    "\n",
    "# NOR Logic Function\n",
    "# with OR and NOT\n",
    "# function calls in sequence\n",
    "def NOR_logicFunction(x):\n",
    "  output_OR = OR_logicFunction(x)\n",
    "  output_NOT = NOT_logicFunction(output_OR)\n",
    "  return output_NOT\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"NOR({}, {}) = {}\".format(0, 1, NOR_logicFunction(test1)))\n",
    "print(\"NOR({}, {}) = {}\".format(1, 1, NOR_logicFunction(test2)))\n",
    "print(\"NOR({}, {}) = {}\".format(0, 0, NOR_logicFunction(test3)))\n",
    "print(\"NOR({}, {}) = {}\".format(1, 0, NOR_logicFunction(test4)))\n",
    "\n",
    "# NAND Logic Function\n",
    "# with AND and NOT\n",
    "# function calls in sequence\n",
    "def NAND_logicFunction(x):\n",
    "  output_AND = AND_logicFunction(x)\n",
    "  output_NOT = NOT_logicFunction(output_AND)\n",
    "  return output_NOT\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"NAND({}, {}) = {}\".format(0, 1, NAND_logicFunction(test1)))\n",
    "print(\"NAND({}, {}) = {}\".format(1, 1, NAND_logicFunction(test2)))\n",
    "print(\"NAND({}, {}) = {}\".format(0, 0, NAND_logicFunction(test3)))\n",
    "print(\"NAND({}, {}) = {}\".format(1, 0, NAND_logicFunction(test4)))\n",
    "\n",
    "# XOR Logic Function\n",
    "# with AND, OR and NOT\n",
    "# function calls in sequence\n",
    "def XOR_logicFunction(x):\n",
    "  y1 = AND_logicFunction(x)\n",
    "  y2 = OR_logicFunction(x)\n",
    "  y3 = NOT_logicFunction(y1)\n",
    "  final_x = np.array([y2, y3])\n",
    "  finalOutput = AND_logicFunction(final_x)\n",
    "  return finalOutput\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))\n",
    "\n",
    "\n",
    "# XNOR Logic Function\n",
    "# with AND, OR and NOT\n",
    "# function calls in sequence\n",
    "def XNOR_logicFunction(x):\n",
    "  y1 = OR_logicFunction(x)\n",
    "  y2 = AND_logicFunction(x)\n",
    "  y3 = NOT_logicFunction(y1)\n",
    "  final_x = np.array([y2, y3])\n",
    "  finalOutput = OR_logicFunction(final_x)\n",
    "  return finalOutput\n",
    "\n",
    "# testing the Perceptron Model\n",
    "test1 = np.array([0, 1])\n",
    "test2 = np.array([1, 1])\n",
    "test3 = np.array([0, 0])\n",
    "test4 = np.array([1, 0])\n",
    "\n",
    "print(\"XNOR({}, {}) = {}\".format(0, 1, XNOR_logicFunction(test1)))\n",
    "print(\"XNOR({}, {}) = {}\".format(1, 1, XNOR_logicFunction(test2)))\n",
    "print(\"XNOR({}, {}) = {}\".format(0, 0, XNOR_logicFunction(test3)))\n",
    "print(\"XNOR({}, {}) = {}\".format(1, 0, XNOR_logicFunction(test4)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNdTbuOjWwTvSLshmlO+8q+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
